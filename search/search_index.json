{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction bentoctl is a command line tool for deploying BentoML packaged ML models as API endpoint on popular cloud platforms. It automates Bento docker image build, generate terraform project, and easily manage deployments. Why bentoctl? Supports major cloud providers: AWS, Azure, Google Cloud, and more. Easy to deploy, update and reproduce model deployments. First class integration with Terraform. Optimized for CI/CD workflow. Extensible with custom operators. High performance serving powered by BentoML Supported platforms: AWS Lambda AWS SageMaker AWS EC2 Google Cloud Run Azure Functions Looking for Kubernetes ? Try out Yatai: Model deployment at scale on Kubernetes . Customize deploy target by creating bentoctl plugin from the deployment operator template . Upcoming: * Google Compute Engine (BentoML 1.0 migration in progress) * Azure Container Instances (BentoML 1.0 migration in progress) * Heroku (BentoML 1.0 migration in progress) * Knative (WIP) How to install Install via pip: pip install --pre bentoctl bentoctl is in pre-release stage, include the --pre to download the latest version Next steps Quickstart Guide walks through a series of steps to deploy a bento to AWS Lambda as API server. Core Concepts explains the core concepts in bentoctl. Operator List lists official operators and their current status. Community To report a bug or suggest a feature request, use GitHub Issues . To receive release announcements, please subscribe to our mailing list or join us on Slack . Contributing There are many ways to contribute to the project: If you have any feedback on the project, share it with the community in #bentoctl channel in slack. Report issues you're facing and \"Thumbs up\" on issues and feature requests that are relevant to you. Investigate bugs and review other developer's pull requests.","title":"Introduction"},{"location":"#introduction","text":"bentoctl is a command line tool for deploying BentoML packaged ML models as API endpoint on popular cloud platforms. It automates Bento docker image build, generate terraform project, and easily manage deployments.","title":"Introduction"},{"location":"#why-bentoctl","text":"Supports major cloud providers: AWS, Azure, Google Cloud, and more. Easy to deploy, update and reproduce model deployments. First class integration with Terraform. Optimized for CI/CD workflow. Extensible with custom operators. High performance serving powered by BentoML Supported platforms: AWS Lambda AWS SageMaker AWS EC2 Google Cloud Run Azure Functions Looking for Kubernetes ? Try out Yatai: Model deployment at scale on Kubernetes . Customize deploy target by creating bentoctl plugin from the deployment operator template . Upcoming: * Google Compute Engine (BentoML 1.0 migration in progress) * Azure Container Instances (BentoML 1.0 migration in progress) * Heroku (BentoML 1.0 migration in progress) * Knative (WIP)","title":"Why bentoctl?"},{"location":"#how-to-install","text":"Install via pip: pip install --pre bentoctl bentoctl is in pre-release stage, include the --pre to download the latest version","title":"How to install"},{"location":"#next-steps","text":"Quickstart Guide walks through a series of steps to deploy a bento to AWS Lambda as API server. Core Concepts explains the core concepts in bentoctl. Operator List lists official operators and their current status.","title":"Next steps"},{"location":"#community","text":"To report a bug or suggest a feature request, use GitHub Issues . To receive release announcements, please subscribe to our mailing list or join us on Slack .","title":"Community"},{"location":"#contributing","text":"There are many ways to contribute to the project: If you have any feedback on the project, share it with the community in #bentoctl channel in slack. Report issues you're facing and \"Thumbs up\" on issues and feature requests that are relevant to you. Investigate bugs and review other developer's pull requests.","title":"Contributing"},{"location":"013-deployment/","text":"AWS Lambda deployment tool AWS Lambda is a great service for quickly deploy service to the cloud for immediate access. It's ability to auto scale resources base on usage make it attractive to user who want to save cost and want to scale base on usage without administrative overhead. Prerequisites An active AWS account configured on the machine with AWS CLI installed and configured Install instruction: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html Configure AWS account instruction: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html AWS SAM CLI (>=1.27). Installation instructions https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html Docker is installed and running on the machine. Install instruction: https://docs.docker.com/install Bento build built with BentoML version 0.13 or below. Deploy IrisClassifier from BentoML quick start guide to AWS Lambda Setup the AWS Lambda deployment a. Clone the operator repo bash git clone https://github.com/bentoml/aws-lambda-deploy.git b. Install the operator requirements bash cd aws-lambda-deploy pip install -r requirements.txt Copy and change the sample config file given and change it according to your deployment specifications. # lambda_config.json { \"region\": \"us-west-1\", \"timeout\": 60, \"memory_size\": 1024 } Create Lambda deployment with the deployment tool. Run deploy script in the command line: ```bash $ BENTO_BUNDLE_PATH=$(bentoml get IrisClassifier:latest --print-location -q) $ python deploy.py $BENTO_BUNDLE_PATH my-lambda-deployment lambda_config.json ``` Get deployment information and status ```bash $ python describe.py my-lambda-deployment # Sample output { \"StackId\": \"arn:aws:cloudformation:us-west-1:192023623294:stack/my-lambda-deployment-stack/29c15040-db7a-11eb-a721-028d528946df\", \"StackName\": \"my-lambda-deployment-stack\", \"StackStatus\": \"CREATE_COMPLETE\", \"CreationTime\": \"07/02/2021, 21:12:09\", \"LastUpdatedTime\": \"07/02/2021, 21:12:20\", \"EndpointUrl\": \"https://j2gm5zn7z9.execute-api.us-west-1.amazonaws.com/Prod\" } ``` Make sample request against deployed service. The url for the endpoint given in the output of the describe command or you can also check the API Gateway through the AWS console. ```bash curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '[[5.1, 3.5, 1.4, 0.2]]' \\ https://j2gm5zn7z9.execute-api.us-west-1.amazonaws.com/Prod/predict Sample output HTTP/2 200 content-type: application/json content-length: 3 date: Sat, 03 Jul 2021 19:14:38 GMT x-amzn-requestid: d3b5f156-0859-4f69-8b53-c60e800bc0aa x-amz-apigw-id: B6GLLECTSK4FY2w= x-amzn-trace-id: Root=1-60e0b714-18a97eb5696cec991c460213;Sampled=0 x-cache: Miss from cloudfront via: 1.1 6af3b573d8970d5db2a4d03354335b85.cloudfront.net (CloudFront) x-amz-cf-pop: SEA19-C3 x-amz-cf-id: ArwZ03gbs6GooNN1fy4mPOgaEpM4h4n9gz2lpLYrHmeXZJuGUJgz0Q== [0]% ``` Delete Lambda deployment bash $ python delete.py my-lambda-deployment Deployment operations Configuration options region : AWS region for Lambda deployment timeout : Timeout per request memory_size : The memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments Create a deployment Use CLI python deploy.py <Bento_bundle_path> <Deployment_name> <Config_JSON default is ./lambda_config.json> Example: MY_BUNDLE_PATH=${bentoml get IrisClassifier:latest --print-location -q) python deploy.py $MY_BUNDLE_PATH my_first_deployment lambda_config.json Use Python API from deploy import deploy_aws_lambda deploy_aws_lambda(BENTO_BUNDLE_PATH, DEPLOYMENT_NAME, CONFIG_JSON) Update a deployment Use CLI python update.py <Bento_bundle_path> <Deployment_name> <Config_JSON> Use Python API from update import update_aws_lambda update_aws_lambda(BENTO_BUNDLE_PATH, DEPLOYMENT_NAME, CONFIG_JSON) Get deployment's status and information Use CLI python describe.py <Deployment_name> <Config_JSON> Use Python API from describe import describe_deployment describe_deployment(DEPLOYMENT_NAME, CONFIG_JSON) Delete deployment Use CLI python delete.py <Deployment_name> <Config_JSON> Use Python API from delete import delete_deployment delete_deployment(DEPLOYMENT_NAME, CONFIG_JSON)","title":"AWS Lambda deployment tool"},{"location":"013-deployment/#aws-lambda-deployment-tool","text":"AWS Lambda is a great service for quickly deploy service to the cloud for immediate access. It's ability to auto scale resources base on usage make it attractive to user who want to save cost and want to scale base on usage without administrative overhead.","title":"AWS Lambda deployment tool"},{"location":"013-deployment/#prerequisites","text":"An active AWS account configured on the machine with AWS CLI installed and configured Install instruction: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html Configure AWS account instruction: https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html AWS SAM CLI (>=1.27). Installation instructions https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html Docker is installed and running on the machine. Install instruction: https://docs.docker.com/install Bento build built with BentoML version 0.13 or below.","title":"Prerequisites"},{"location":"013-deployment/#deploy-irisclassifier-from-bentoml-quick-start-guide-to-aws-lambda","text":"Setup the AWS Lambda deployment a. Clone the operator repo bash git clone https://github.com/bentoml/aws-lambda-deploy.git b. Install the operator requirements bash cd aws-lambda-deploy pip install -r requirements.txt Copy and change the sample config file given and change it according to your deployment specifications. # lambda_config.json { \"region\": \"us-west-1\", \"timeout\": 60, \"memory_size\": 1024 } Create Lambda deployment with the deployment tool. Run deploy script in the command line: ```bash $ BENTO_BUNDLE_PATH=$(bentoml get IrisClassifier:latest --print-location -q) $ python deploy.py $BENTO_BUNDLE_PATH my-lambda-deployment lambda_config.json ``` Get deployment information and status ```bash $ python describe.py my-lambda-deployment # Sample output { \"StackId\": \"arn:aws:cloudformation:us-west-1:192023623294:stack/my-lambda-deployment-stack/29c15040-db7a-11eb-a721-028d528946df\", \"StackName\": \"my-lambda-deployment-stack\", \"StackStatus\": \"CREATE_COMPLETE\", \"CreationTime\": \"07/02/2021, 21:12:09\", \"LastUpdatedTime\": \"07/02/2021, 21:12:20\", \"EndpointUrl\": \"https://j2gm5zn7z9.execute-api.us-west-1.amazonaws.com/Prod\" } ``` Make sample request against deployed service. The url for the endpoint given in the output of the describe command or you can also check the API Gateway through the AWS console. ```bash curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '[[5.1, 3.5, 1.4, 0.2]]' \\ https://j2gm5zn7z9.execute-api.us-west-1.amazonaws.com/Prod/predict","title":"Deploy IrisClassifier from BentoML quick start guide to AWS Lambda"},{"location":"013-deployment/#sample-output","text":"HTTP/2 200 content-type: application/json content-length: 3 date: Sat, 03 Jul 2021 19:14:38 GMT x-amzn-requestid: d3b5f156-0859-4f69-8b53-c60e800bc0aa x-amz-apigw-id: B6GLLECTSK4FY2w= x-amzn-trace-id: Root=1-60e0b714-18a97eb5696cec991c460213;Sampled=0 x-cache: Miss from cloudfront via: 1.1 6af3b573d8970d5db2a4d03354335b85.cloudfront.net (CloudFront) x-amz-cf-pop: SEA19-C3 x-amz-cf-id: ArwZ03gbs6GooNN1fy4mPOgaEpM4h4n9gz2lpLYrHmeXZJuGUJgz0Q== [0]% ``` Delete Lambda deployment bash $ python delete.py my-lambda-deployment","title":"Sample output"},{"location":"013-deployment/#deployment-operations","text":"","title":"Deployment operations"},{"location":"013-deployment/#configuration-options","text":"region : AWS region for Lambda deployment timeout : Timeout per request memory_size : The memory for your function, set a value between 128 MB and 10,240 MB in 1-MB increments","title":"Configuration options"},{"location":"013-deployment/#create-a-deployment","text":"Use CLI python deploy.py <Bento_bundle_path> <Deployment_name> <Config_JSON default is ./lambda_config.json> Example: MY_BUNDLE_PATH=${bentoml get IrisClassifier:latest --print-location -q) python deploy.py $MY_BUNDLE_PATH my_first_deployment lambda_config.json Use Python API from deploy import deploy_aws_lambda deploy_aws_lambda(BENTO_BUNDLE_PATH, DEPLOYMENT_NAME, CONFIG_JSON)","title":"Create a deployment"},{"location":"013-deployment/#update-a-deployment","text":"Use CLI python update.py <Bento_bundle_path> <Deployment_name> <Config_JSON> Use Python API from update import update_aws_lambda update_aws_lambda(BENTO_BUNDLE_PATH, DEPLOYMENT_NAME, CONFIG_JSON)","title":"Update a deployment"},{"location":"013-deployment/#get-deployments-status-and-information","text":"Use CLI python describe.py <Deployment_name> <Config_JSON> Use Python API from describe import describe_deployment describe_deployment(DEPLOYMENT_NAME, CONFIG_JSON)","title":"Get deployment's status and information"},{"location":"013-deployment/#delete-deployment","text":"Use CLI python delete.py <Deployment_name> <Config_JSON> Use Python API from delete import delete_deployment delete_deployment(DEPLOYMENT_NAME, CONFIG_JSON)","title":"Delete deployment"},{"location":"cli-reference/","text":"bentoctl Bentoctl - Fast model deployment on any cloud platform bentoctl is a CLI tool for deploying your machine-learning models to any cloud platforms and serving predictions via REST APIs. It is built on top of BentoML: the unified model serving framework, and makes it easy to bring any BentoML packaged model to production. Usage: bentoctl [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. --help Show this message and exit. apply [Experimental] Apply the generated template file to create/update the deployment. Usage: bentoctl apply [OPTIONS] Options: -f, --deployment-config-file TEXT path to deployment_config file --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. build Build the Docker image for the given deployment config file and bento. Usage: bentoctl build [OPTIONS] Options: -b, --bento-tag TEXT Bento tag to use for deployment. [required] -f, --deployment-config-file TEXT path to deployment_config file --dry-run Dry run --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. destroy Destroy all the resources created and remove the registry. Usage: bentoctl destroy [OPTIONS] Options: -f, --deployment-config-file TEXT path to deployment_config file --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. generate Generate template files for deployment. Usage: bentoctl generate [OPTIONS] Options: -f, --deployment-config-file TEXT path to deployment config file --save-path DIRECTORY Path to save the deployment config file. --values-only create/update the values file only. --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. init Start the interactive deployment config builder file. Usage: bentoctl init [OPTIONS] Options: --do-not-generate Generate template files based on the provided operator. --save-path DIRECTORY Path to save the deployment config file. --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. operator Sub-commands to install, list, remove and update operators. To see the list of all the operators available and their comparisons check out . Usage: bentoctl operator [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit. install install operators. There are 5 ways to install an operator into bentoctl and they are - Interactive Mode: lists all official operators for user to choose from. Just type bentoctl install to enter this mode. Official operator: you can pass the name of one of the official operators and the tool with fetch it for you. You can see the list of official operators . eg. bentoctl install aws-lambda Path: If you have the operator locally, either because you are building our own operator or if cloning the operator from some other remote repository (other than github) then you can just pass the path after the install command and it will register the local operator for you. This is a special case since the operator will not have an associated URL with it and hence cannot be updated using the tool. Github Repo: this should be in the format repo_owner/repo_name[:repo_branch] . eg: bentoctl install bentoml/aws-lambda-repo Git Url: of the form https://[\\w]+.git. eg: bentoctl install https://github.com/bentoml/aws-lambda-deploy.git Usage: bentoctl operator install [OPTIONS] [NAME] Options: --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. list List all the available operators. Lists the operator, the path from where you can access operator locally and if the operator was pulled from github, the github URL is also shown. Usage: bentoctl operator list [OPTIONS] Options: --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. remove Remove operators. This will remove the operator from the list and also remove the local codebase. Pass the flag --keep-locally to keep the operator codebase in the local director. Usage: bentoctl operator remove [OPTIONS] NAME Options: -y skip the prompt asking if you are sure. --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit. update Update an operator given its name. This only works for operators that have a URL associated with them. When passed the name of an available operator it goes and fetches the latest code from the Github repo and update the local codebase with it. Usage: bentoctl operator update [OPTIONS] NAME Options: --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"CLI reference"},{"location":"cli-reference/#bentoctl","text":"Bentoctl - Fast model deployment on any cloud platform bentoctl is a CLI tool for deploying your machine-learning models to any cloud platforms and serving predictions via REST APIs. It is built on top of BentoML: the unified model serving framework, and makes it easy to bring any BentoML packaged model to production. Usage: bentoctl [OPTIONS] COMMAND [ARGS]... Options: --version Show the version and exit. --help Show this message and exit.","title":"bentoctl"},{"location":"cli-reference/#apply","text":"[Experimental] Apply the generated template file to create/update the deployment. Usage: bentoctl apply [OPTIONS] Options: -f, --deployment-config-file TEXT path to deployment_config file --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"apply"},{"location":"cli-reference/#build","text":"Build the Docker image for the given deployment config file and bento. Usage: bentoctl build [OPTIONS] Options: -b, --bento-tag TEXT Bento tag to use for deployment. [required] -f, --deployment-config-file TEXT path to deployment_config file --dry-run Dry run --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"build"},{"location":"cli-reference/#destroy","text":"Destroy all the resources created and remove the registry. Usage: bentoctl destroy [OPTIONS] Options: -f, --deployment-config-file TEXT path to deployment_config file --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"destroy"},{"location":"cli-reference/#generate","text":"Generate template files for deployment. Usage: bentoctl generate [OPTIONS] Options: -f, --deployment-config-file TEXT path to deployment config file --save-path DIRECTORY Path to save the deployment config file. --values-only create/update the values file only. --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"generate"},{"location":"cli-reference/#init","text":"Start the interactive deployment config builder file. Usage: bentoctl init [OPTIONS] Options: --do-not-generate Generate template files based on the provided operator. --save-path DIRECTORY Path to save the deployment config file. --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"init"},{"location":"cli-reference/#operator","text":"Sub-commands to install, list, remove and update operators. To see the list of all the operators available and their comparisons check out . Usage: bentoctl operator [OPTIONS] COMMAND [ARGS]... Options: --help Show this message and exit.","title":"operator"},{"location":"cli-reference/#install","text":"install operators. There are 5 ways to install an operator into bentoctl and they are - Interactive Mode: lists all official operators for user to choose from. Just type bentoctl install to enter this mode. Official operator: you can pass the name of one of the official operators and the tool with fetch it for you. You can see the list of official operators . eg. bentoctl install aws-lambda Path: If you have the operator locally, either because you are building our own operator or if cloning the operator from some other remote repository (other than github) then you can just pass the path after the install command and it will register the local operator for you. This is a special case since the operator will not have an associated URL with it and hence cannot be updated using the tool. Github Repo: this should be in the format repo_owner/repo_name[:repo_branch] . eg: bentoctl install bentoml/aws-lambda-repo Git Url: of the form https://[\\w]+.git. eg: bentoctl install https://github.com/bentoml/aws-lambda-deploy.git Usage: bentoctl operator install [OPTIONS] [NAME] Options: --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"install"},{"location":"cli-reference/#list","text":"List all the available operators. Lists the operator, the path from where you can access operator locally and if the operator was pulled from github, the github URL is also shown. Usage: bentoctl operator list [OPTIONS] Options: --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"list"},{"location":"cli-reference/#remove","text":"Remove operators. This will remove the operator from the list and also remove the local codebase. Pass the flag --keep-locally to keep the operator codebase in the local director. Usage: bentoctl operator remove [OPTIONS] NAME Options: -y skip the prompt asking if you are sure. --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"remove"},{"location":"cli-reference/#update","text":"Update an operator given its name. This only works for operators that have a URL associated with them. When passed the name of an available operator it goes and fetches the latest code from the Github repo and update the local codebase with it. Usage: bentoctl operator update [OPTIONS] NAME Options: --verbose, --debug Show debug logs when running the command --do-not-track Do not send uage info --help Show this message and exit.","title":"update"},{"location":"core-concepts/","text":"Core Concepts Bentoctl performs two types of operations: bentoctl builds deployable images or artifacts based on the cloud services' requirements. bentoctl managages cloud service deployment with Terraform In the following sections, we will discuss the concepts of operators and deployment configurations. Operators Operators are plugins that interact with the external services, typcially a cloud service. It abstracts the specifics implmentation details of the external service and provides an unified interface for bentoctl. The operator provide 2 core actions that are: Create deployable image or artifact base on the cloud service's requirements. Generate Terraform projects based on the deployment configuration. The operator also provides a set of schema that bentoctl uses to validate the deployment configuration. Operators page provides more details on supported platforms and their current status. The operator designed to be customizable and extensible. Users can install non-offical operators from git url or from a local file path. Users can create their own operator from the operator template on Github . Deployment Configuration bentoctl uses deployment configuration to specify the deployment properties. The deployment configuration stores in local system using yaml format An Anatomy of a Deployment Config file Here is a sample deployment config for the EC2 operator. 1 :- api_version specifies deployment configuration version. 2 :- name Deployment's name. bentoctl recommends to keep deployment name unqiue within each operator to avoid any potential issues. 3 :- operator Operator used for the deployment. bentoctl will automatically install official operator if it is not installed. 4 :- template The template for the deployment. It determines what the generated terraform project will look like. 5 :- spec specifics the deployment details. The deployment detail options are provided by the operator listed. 6 :- spec.* - Deployment options specific to the operator.","title":"Core Concepts"},{"location":"core-concepts/#core-concepts","text":"Bentoctl performs two types of operations: bentoctl builds deployable images or artifacts based on the cloud services' requirements. bentoctl managages cloud service deployment with Terraform In the following sections, we will discuss the concepts of operators and deployment configurations.","title":"Core Concepts"},{"location":"core-concepts/#operators","text":"Operators are plugins that interact with the external services, typcially a cloud service. It abstracts the specifics implmentation details of the external service and provides an unified interface for bentoctl. The operator provide 2 core actions that are: Create deployable image or artifact base on the cloud service's requirements. Generate Terraform projects based on the deployment configuration. The operator also provides a set of schema that bentoctl uses to validate the deployment configuration. Operators page provides more details on supported platforms and their current status. The operator designed to be customizable and extensible. Users can install non-offical operators from git url or from a local file path. Users can create their own operator from the operator template on Github .","title":"Operators"},{"location":"core-concepts/#deployment-configuration","text":"bentoctl uses deployment configuration to specify the deployment properties. The deployment configuration stores in local system using yaml format","title":"Deployment Configuration"},{"location":"core-concepts/#an-anatomy-of-a-deployment-config-file","text":"Here is a sample deployment config for the EC2 operator. 1 :- api_version specifies deployment configuration version. 2 :- name Deployment's name. bentoctl recommends to keep deployment name unqiue within each operator to avoid any potential issues. 3 :- operator Operator used for the deployment. bentoctl will automatically install official operator if it is not installed. 4 :- template The template for the deployment. It determines what the generated terraform project will look like. 5 :- spec specifics the deployment details. The deployment detail options are provided by the operator listed. 6 :- spec.* - Deployment options specific to the operator.","title":"An Anatomy of a Deployment Config file"},{"location":"operator-list/","text":"Operator List Official Operators Operator Name Github Repo Status [ Migrated to 1.0] Guides aws-lambda https://github.com/bentoml/aws-lambda-deploy/tree/main \u2705 aws-sagemaker https://github.com/bentoml/aws-sagemaker-deploy \u2705 aws-ec2 https://github.com/bentoml/aws-ec2-deploy \u274c google-compute-engine https://github.com/bentoml/google-compute-engine-deploy \u274c google-cloud-run https://github.com/bentoml/google-cloud-run-deploy \u274c azure-functions https://github.com/bentoml/azure-functions-deploy \u274c azure-container-instances https://github.com/bentoml/azure-container-instances-deploy \u274c Heroku https://github.com/bentoml/heroku-deploy \u274c Community Operators","title":"Operators"},{"location":"operator-list/#operator-list","text":"","title":"Operator List"},{"location":"operator-list/#official-operators","text":"Operator Name Github Repo Status [ Migrated to 1.0] Guides aws-lambda https://github.com/bentoml/aws-lambda-deploy/tree/main \u2705 aws-sagemaker https://github.com/bentoml/aws-sagemaker-deploy \u2705 aws-ec2 https://github.com/bentoml/aws-ec2-deploy \u274c google-compute-engine https://github.com/bentoml/google-compute-engine-deploy \u274c google-cloud-run https://github.com/bentoml/google-cloud-run-deploy \u274c azure-functions https://github.com/bentoml/azure-functions-deploy \u274c azure-container-instances https://github.com/bentoml/azure-container-instances-deploy \u274c Heroku https://github.com/bentoml/heroku-deploy \u274c","title":"Official Operators"},{"location":"operator-list/#community-operators","text":"","title":"Community Operators"},{"location":"quickstart/","text":"Quickstart This guide walks through the steps of building a bento and deploying it to AWS Lambda. It will use the iris classifier bento with classify api endpoint created in the BentoML quickstart guide, and then use bentoctl to deploy to AWS lambda. Prerequisites Bentoml - BentoML version 1.0 and greater. Please follow the Installation guide . Terraform - Terraform is a tool for building, configuring, and managing infrastructure. AWS CLI installed and configured with an AWS account with permission to the Cloudformation, Lamba, API Gateway and ECR. Please follow the Installation guide . Step 1: Create a bento Note: Skip to step 2, if you already built a bento with the BentoML 1.0 quick start guide. Follow the instructions from the BentoML\u2019s quickstart guide to build a bento. Step 2: Verify bento To verify the bento, run bentoml list to display the available bentos in your local bento store: > bentoml list Tag Size Creation Time Path iris_classifier:btzv5wfv665trhcu 19.78 KiB 2022-04-06 22:15:26 ~/bentoml/bentos/iris_classifier/btzv5wfv665trhcu Step 3: Install aws-lambda operator bentoctl has operators that help deploy bentos to different cloud services. Operator is a plugin that interacts with the cloud service to perform the bentoctl commands. The operator is responsible for creating and configuring the resources for deploying to the cloud service. Learn more from the Core Concepts page. This guide uses the official aws-lambda operator to deploy and manage deployments. Run the following command to install the operator and its dependencies to your local system bentoctl operator install aws-lambda Step 4: Generate deployment files bentoctl uses the deployment configuration to specify the deployment properties and generate Terraform project from that. Deployment Configuration is a YAML file that specifies properties of the deployment like which bento service to deploy, what operator to use and other configurations. Learn more from the Core Concepts page bentoctl offers an interactive cli command, bentoctl init , to guide users to generate the deployment configuration and the terraform projects. bentoctl init Bentoctl Interactive Deployment Config Builder Welcome! You are now in interactive mode. This mode will help you setup the deployment_config.yaml file required for deployment. Fill out the appropriate values for the fields. (deployment config will be saved to: ./deployment_config.yaml) api_version: v1 name: quickstart operator: aws-lambda template: terraform spec: region: us-west-1 timeout: 10 memory_size: 512 filename for deployment_config [deployment_config.yaml]: deployment config generated to: deployment_config.yaml \u2728 generated template files. - bentoctl.tfvars - main.tf Step 5: Build image for deployment Bentoctl prepares the docker image base on the cloud service's requirements, and then build and push the docker image to the deployment registry. bentoctl build -b iris_classifier:btzv5wfv665trhcu -f deployment_config.yaml Step 1/20 : FROM bentoml/bento-server:1.0.0a7-python3.7-debian-runtime ---> dde7b88477b1 Step 2/20 : ARG UID=1034 ---> Running in b8f4ae1d8b08 ---> e6c313c8d9ea Step 3/20 : ARG GID=1034 .... Step 20/20 : ENTRYPOINT [ \"/opt/conda/bin/python\", \"-m\", \"awslambdaric\" ] ---> Running in 4e56057f3b18 ---> dca82bca9034 Successfully built dca82bca9034 Successfully tagged aws-lambda-iris_classifier:btzv5wfv665trhcu \ud83d\udd28 Image build! The push refers to repository [192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart] btzv5wfv665trhcu: digest: sha256:ffcd120f7629122cf5cd95664e4fd28e9a50e799be7bb23f0b5b03f14ca5c672 size: 3253 32096534b881: Pushed f709d8f0f57d: Pushed 7d30486f5c78: Pushed ... c1065d45b872: Pushed \ud83d\ude80 Image pushed! \u2728 generated template files. - bentoctl.tfvars The push refers to repository [192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart] Afterward, bentoctl will update the terraform variables with the docker image information Step 6: Deploy to Lambda Initialize Terraform project terraform init Apply Terraform changes terraform apply -var-file=bentoctl.tfvars --auto-approve aws_iam_role.lambda_exec: Creating... aws_apigatewayv2_api.lambda: Creating... aws_apigatewayv2_api.lambda: Creation complete after 1s [id=ka8h2p2yfh] aws_cloudwatch_log_group.api_gw: Creating... aws_cloudwatch_log_group.api_gw: Creation complete after 0s [id=/aws/api_gw/quickstart-gw] aws_apigatewayv2_stage.lambda: Creating... aws_iam_role.lambda_exec: Creation complete after 3s [id=quickstart-iam] aws_iam_role_policy_attachment.lambda_policy: Creating... aws_lambda_function.fn: Creating... aws_apigatewayv2_stage.lambda: Creation complete after 2s [id=$default] aws_iam_role_policy_attachment.lambda_policy: Creation complete after 1s [id=quickstart-iam-20220414203448384500000001] aws_lambda_function.fn: Still creating... [10s elapsed] aws_lambda_function.fn: Still creating... [20s elapsed] aws_lambda_function.fn: Still creating... [30s elapsed] aws_lambda_function.fn: Still creating... [40s elapsed] aws_lambda_function.fn: Creation complete after 41s [id=quickstart-function] aws_lambda_permission.api_gw: Creating... aws_cloudwatch_log_group.lg: Creating... aws_apigatewayv2_integration.lambda: Creating... aws_lambda_permission.api_gw: Creation complete after 0s [id=AllowExecutionFromAPIGateway] aws_cloudwatch_log_group.lg: Creation complete after 0s [id=/aws/lambda/quickstart-function] aws_apigatewayv2_integration.lambda: Creation complete after 1s [id=8gumjws] aws_apigatewayv2_route.root: Creating... aws_apigatewayv2_route.services: Creating... aws_apigatewayv2_route.root: Creation complete after 0s [id=jjp5f23] aws_apigatewayv2_route.services: Creation complete after 0s [id=8n57a1d] Apply complete! Resources: 11 added, 0 changed, 0 destroyed. Outputs: base_url = \"https://ka8h2p2yfh.execute-api.us-west-1.amazonaws.com/\" function_name = \"quickstart-function\" image_tag = \"192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart:btzv5wfv665trhcu\" step 7: Make a prediction The iris_classifier uses the /classify endpoint for receiving requests so the full URL for the classifier will be in the form {EndpointUrl}/classify URL=$(terraform output -json | jq -r .base_url.value)classify curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '[5.1, 3.5, 1.4, 0.2]' \\ $URL HTTP/2 200 date: Thu, 14 Apr 2022 23:02:45 GMT content-type: application/json content-length: 1 apigw-requestid: Ql8zbicdSK4EM5g= 0% Step 8: Cleanup Deployment To delete deployment, run the terraform destroy command terraform destroy -var-file=bentoctl.tfvars --auto-approve","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"This guide walks through the steps of building a bento and deploying it to AWS Lambda. It will use the iris classifier bento with classify api endpoint created in the BentoML quickstart guide, and then use bentoctl to deploy to AWS lambda.","title":"Quickstart"},{"location":"quickstart/#prerequisites","text":"Bentoml - BentoML version 1.0 and greater. Please follow the Installation guide . Terraform - Terraform is a tool for building, configuring, and managing infrastructure. AWS CLI installed and configured with an AWS account with permission to the Cloudformation, Lamba, API Gateway and ECR. Please follow the Installation guide .","title":"Prerequisites"},{"location":"quickstart/#step-1-create-a-bento","text":"Note: Skip to step 2, if you already built a bento with the BentoML 1.0 quick start guide. Follow the instructions from the BentoML\u2019s quickstart guide to build a bento.","title":"Step 1: Create a bento"},{"location":"quickstart/#step-2-verify-bento","text":"To verify the bento, run bentoml list to display the available bentos in your local bento store: > bentoml list Tag Size Creation Time Path iris_classifier:btzv5wfv665trhcu 19.78 KiB 2022-04-06 22:15:26 ~/bentoml/bentos/iris_classifier/btzv5wfv665trhcu","title":"Step 2: Verify bento"},{"location":"quickstart/#step-3-install-aws-lambda-operator","text":"bentoctl has operators that help deploy bentos to different cloud services. Operator is a plugin that interacts with the cloud service to perform the bentoctl commands. The operator is responsible for creating and configuring the resources for deploying to the cloud service. Learn more from the Core Concepts page. This guide uses the official aws-lambda operator to deploy and manage deployments. Run the following command to install the operator and its dependencies to your local system bentoctl operator install aws-lambda","title":"Step 3: Install aws-lambda operator"},{"location":"quickstart/#step-4-generate-deployment-files","text":"bentoctl uses the deployment configuration to specify the deployment properties and generate Terraform project from that. Deployment Configuration is a YAML file that specifies properties of the deployment like which bento service to deploy, what operator to use and other configurations. Learn more from the Core Concepts page bentoctl offers an interactive cli command, bentoctl init , to guide users to generate the deployment configuration and the terraform projects. bentoctl init Bentoctl Interactive Deployment Config Builder Welcome! You are now in interactive mode. This mode will help you setup the deployment_config.yaml file required for deployment. Fill out the appropriate values for the fields. (deployment config will be saved to: ./deployment_config.yaml) api_version: v1 name: quickstart operator: aws-lambda template: terraform spec: region: us-west-1 timeout: 10 memory_size: 512 filename for deployment_config [deployment_config.yaml]: deployment config generated to: deployment_config.yaml \u2728 generated template files. - bentoctl.tfvars - main.tf","title":"Step 4: Generate deployment files"},{"location":"quickstart/#step-5-build-image-for-deployment","text":"Bentoctl prepares the docker image base on the cloud service's requirements, and then build and push the docker image to the deployment registry. bentoctl build -b iris_classifier:btzv5wfv665trhcu -f deployment_config.yaml Step 1/20 : FROM bentoml/bento-server:1.0.0a7-python3.7-debian-runtime ---> dde7b88477b1 Step 2/20 : ARG UID=1034 ---> Running in b8f4ae1d8b08 ---> e6c313c8d9ea Step 3/20 : ARG GID=1034 .... Step 20/20 : ENTRYPOINT [ \"/opt/conda/bin/python\", \"-m\", \"awslambdaric\" ] ---> Running in 4e56057f3b18 ---> dca82bca9034 Successfully built dca82bca9034 Successfully tagged aws-lambda-iris_classifier:btzv5wfv665trhcu \ud83d\udd28 Image build! The push refers to repository [192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart] btzv5wfv665trhcu: digest: sha256:ffcd120f7629122cf5cd95664e4fd28e9a50e799be7bb23f0b5b03f14ca5c672 size: 3253 32096534b881: Pushed f709d8f0f57d: Pushed 7d30486f5c78: Pushed ... c1065d45b872: Pushed \ud83d\ude80 Image pushed! \u2728 generated template files. - bentoctl.tfvars The push refers to repository [192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart] Afterward, bentoctl will update the terraform variables with the docker image information","title":"Step 5: Build image for deployment"},{"location":"quickstart/#step-6-deploy-to-lambda","text":"Initialize Terraform project terraform init Apply Terraform changes terraform apply -var-file=bentoctl.tfvars --auto-approve aws_iam_role.lambda_exec: Creating... aws_apigatewayv2_api.lambda: Creating... aws_apigatewayv2_api.lambda: Creation complete after 1s [id=ka8h2p2yfh] aws_cloudwatch_log_group.api_gw: Creating... aws_cloudwatch_log_group.api_gw: Creation complete after 0s [id=/aws/api_gw/quickstart-gw] aws_apigatewayv2_stage.lambda: Creating... aws_iam_role.lambda_exec: Creation complete after 3s [id=quickstart-iam] aws_iam_role_policy_attachment.lambda_policy: Creating... aws_lambda_function.fn: Creating... aws_apigatewayv2_stage.lambda: Creation complete after 2s [id=$default] aws_iam_role_policy_attachment.lambda_policy: Creation complete after 1s [id=quickstart-iam-20220414203448384500000001] aws_lambda_function.fn: Still creating... [10s elapsed] aws_lambda_function.fn: Still creating... [20s elapsed] aws_lambda_function.fn: Still creating... [30s elapsed] aws_lambda_function.fn: Still creating... [40s elapsed] aws_lambda_function.fn: Creation complete after 41s [id=quickstart-function] aws_lambda_permission.api_gw: Creating... aws_cloudwatch_log_group.lg: Creating... aws_apigatewayv2_integration.lambda: Creating... aws_lambda_permission.api_gw: Creation complete after 0s [id=AllowExecutionFromAPIGateway] aws_cloudwatch_log_group.lg: Creation complete after 0s [id=/aws/lambda/quickstart-function] aws_apigatewayv2_integration.lambda: Creation complete after 1s [id=8gumjws] aws_apigatewayv2_route.root: Creating... aws_apigatewayv2_route.services: Creating... aws_apigatewayv2_route.root: Creation complete after 0s [id=jjp5f23] aws_apigatewayv2_route.services: Creation complete after 0s [id=8n57a1d] Apply complete! Resources: 11 added, 0 changed, 0 destroyed. Outputs: base_url = \"https://ka8h2p2yfh.execute-api.us-west-1.amazonaws.com/\" function_name = \"quickstart-function\" image_tag = \"192023623294.dkr.ecr.us-west-1.amazonaws.com/quickstart:btzv5wfv665trhcu\"","title":"Step 6: Deploy to Lambda"},{"location":"quickstart/#step-7-make-a-prediction","text":"The iris_classifier uses the /classify endpoint for receiving requests so the full URL for the classifier will be in the form {EndpointUrl}/classify URL=$(terraform output -json | jq -r .base_url.value)classify curl -i \\ --header \"Content-Type: application/json\" \\ --request POST \\ --data '[5.1, 3.5, 1.4, 0.2]' \\ $URL HTTP/2 200 date: Thu, 14 Apr 2022 23:02:45 GMT content-type: application/json content-length: 1 apigw-requestid: Ql8zbicdSK4EM5g= 0%","title":"step 7: Make a prediction"},{"location":"quickstart/#step-8-cleanup-deployment","text":"To delete deployment, run the terraform destroy command terraform destroy -var-file=bentoctl.tfvars --auto-approve","title":"Step 8: Cleanup Deployment"}]}